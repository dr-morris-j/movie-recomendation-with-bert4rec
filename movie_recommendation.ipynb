{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElRzQdTOK_Js"
      },
      "outputs": [],
      "source": [
        "%pip install torch\n",
        "%pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True, timeout_ms=120000)"
      ],
      "metadata": {
        "id": "BnlOy-H3LBUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/movie_recommendation/movie_dataset_bert/\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtmpukKmLKYV",
        "outputId": "18fcaa79-b4af-41f8-f2de-ed985640a443"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/movie_recommendation/movie_dataset_bert\n",
            "genome-scores.csv  links.csv   ratings.csv  tags.csv\n",
            "genome-tags.csv    movies.csv  README.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn as nn\n",
        "\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import Linear\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "eDdJ2xF1LhoF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_accuracy(y_pred: torch.Tensor, y_true: torch.Tensor, mask: torch.Tensor):\n",
        "\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "    y_true = torch.masked_select(y_true, mask)\n",
        "    predicted = torch.masked_select(predicted, mask)\n",
        "\n",
        "    acc = (y_true == predicted).double().mean()\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def masked_ce(y_pred, y_true, mask):\n",
        "\n",
        "    loss = F.cross_entropy(y_pred, y_true, reduction=\"none\")\n",
        "\n",
        "    loss = loss * mask\n",
        "\n",
        "    return loss.sum() / (mask.sum() + 1e-8)\n",
        "\n",
        "\n",
        "class Recommender(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        channels=128,\n",
        "        cap=0,\n",
        "        mask=1,\n",
        "        dropout=0.4,\n",
        "        lr=1e-4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cap = cap\n",
        "        self.mask = mask\n",
        "\n",
        "        self.lr = lr\n",
        "        self.dropout = dropout\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.item_embeddings = torch.nn.Embedding(\n",
        "            self.vocab_size, embedding_dim=channels\n",
        "        )\n",
        "\n",
        "        self.input_pos_embedding = torch.nn.Embedding(512, embedding_dim=channels)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=channels, nhead=4, dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "\n",
        "        self.linear_out = Linear(channels, self.vocab_size)\n",
        "\n",
        "        self.do = nn.Dropout(p=self.dropout)\n",
        "\n",
        "    def encode_src(self, src_items):\n",
        "        src_items = self.item_embeddings(src_items)\n",
        "\n",
        "        batch_size, in_sequence_len = src_items.size(0), src_items.size(1)\n",
        "        pos_encoder = (\n",
        "            torch.arange(0, in_sequence_len, device=src_items.device)\n",
        "            .unsqueeze(0)\n",
        "            .repeat(batch_size, 1)\n",
        "        )\n",
        "        pos_encoder = self.input_pos_embedding(pos_encoder)\n",
        "\n",
        "        src_items += pos_encoder\n",
        "\n",
        "        src = src_items.permute(1, 0, 2)\n",
        "\n",
        "        src = self.encoder(src)\n",
        "\n",
        "        return src.permute(1, 0, 2)\n",
        "\n",
        "    def forward(self, src_items):\n",
        "\n",
        "        src = self.encode_src(src_items)\n",
        "\n",
        "        out = self.linear_out(src)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, batch_index):\n",
        "        src_items, y_true = batch\n",
        "\n",
        "        y_pred = self(src_items)\n",
        "\n",
        "        y_pred = y_pred.view(-1, y_pred.size(2))\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        src_items = src_items.view(-1)\n",
        "        mask = src_items == self.mask\n",
        "\n",
        "        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_accuracy', accuracy)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_index):\n",
        "        src_items, y_true = batch\n",
        "\n",
        "        y_pred = self(src_items)\n",
        "\n",
        "        y_pred = y_pred.view(-1, y_pred.size(2))\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        src_items = src_items.view(-1)\n",
        "        mask = src_items == self.mask\n",
        "\n",
        "        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "\n",
        "        self.log('valid_loss', loss)\n",
        "        self.log('valid_accuracy', accuracy)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_index):\n",
        "        src_items, y_true = batch\n",
        "\n",
        "        y_pred = self(src_items)\n",
        "\n",
        "        y_pred = y_pred.view(-1, y_pred.size(2))\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        src_items = src_items.view(-1)\n",
        "        mask = src_items == self.mask\n",
        "\n",
        "        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_accuracy', accuracy)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, patience=10, factor=0.1\n",
        "        )\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': scheduler,\n",
        "            'monitor': 'valid_loss',\n",
        "        }"
      ],
      "metadata": {
        "id": "1Ki7hzomMA1O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_list(l1, p=0.8):\n",
        "\n",
        "    l1 = [a if random.random() < p else MASK for a in l1]\n",
        "\n",
        "    return l1\n",
        "\n",
        "\n",
        "def mask_last_elements_list(l1, val_context_size: int = 5):\n",
        "\n",
        "    l1 = l1[:-val_context_size] + mask_list(l1[-val_context_size:], p=0.5)\n",
        "\n",
        "    return l1\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, groups, group_by, split, history_size=120):\n",
        "        self.groups = groups\n",
        "        self.group_by = group_by\n",
        "        self.split = split\n",
        "        self.history_size = history_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.groups)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        group = self.groups[idx]\n",
        "\n",
        "        df = self.group_by.get_group(group)\n",
        "\n",
        "        context = get_context(df, split=self.split, context_size=self.history_size)\n",
        "\n",
        "        trg_items = context['movieId_mapped'].tolist()\n",
        "\n",
        "        if self.split == 'train':\n",
        "            src_items = mask_list(trg_items)\n",
        "        else:\n",
        "            src_items = mask_last_elements_list(trg_items)\n",
        "\n",
        "        pad_mode = 'left' if random.random() < 0.5 else 'right'\n",
        "        trg_items = pad_list(trg_items, history_size=self.history_size, mode=pad_mode)\n",
        "        src_items = pad_list(src_items, history_size=self.history_size, mode=pad_mode)\n",
        "\n",
        "        src_items = torch.tensor(src_items, dtype=torch.long)\n",
        "\n",
        "        trg_items = torch.tensor(trg_items, dtype=torch.long)\n",
        "\n",
        "        return src_items, trg_items\n",
        "\n",
        "\n",
        "def train(\n",
        "    data_csv_path: str,\n",
        "    log_dir: str = 'recommender_logs',\n",
        "    model_dir: str = 'recommender_models',\n",
        "    batch_size: int = 32,\n",
        "    epochs: int = 2000,\n",
        "    history_size: int = 120,\n",
        "):\n",
        "    data = pd.read_csv(data_csv_path)\n",
        "\n",
        "    data.sort_values(by='timestamp', inplace=True)\n",
        "\n",
        "    data, mapping, inverse_mapping = map_column(data, col_name='movieId')\n",
        "\n",
        "    group_by_trains = data.groupby(by='userId')\n",
        "\n",
        "    groups = list(group_by_trains.groups)\n",
        "\n",
        "    train_data = Dataset(groups=groups, group_by=group_by_trains, split='train', history_size=history_size,)\n",
        "    val_data = Dataset(groups=groups, group_by=group_by_trains, split='val', history_size=history_size,)\n",
        "\n",
        "    print(f'len(train_data): {len(train_data)}')\n",
        "    print(f'len(val_data): {len(val_data)}')\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=10,\n",
        "        shuffle=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=10,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    model = Recommender(\n",
        "        vocab_size=len(mapping) + 2,\n",
        "        lr=1e-4,\n",
        "        dropout=0.3,\n",
        "    )\n",
        "\n",
        "    logger = TensorBoardLogger(\n",
        "        save_dir=log_dir,\n",
        "    )\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor='valid_loss',\n",
        "        mode='min',\n",
        "        dirpath=model_dir,\n",
        "        filename='recommender',\n",
        "    )\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=epochs,\n",
        "        gpus=1,\n",
        "        logger=logger,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "    result_val = trainer.test(test_dataloaders=val_loader)\n",
        "\n",
        "    output_json = {\n",
        "        'val_loss': result_val[0]['test_loss'],\n",
        "        'best_model_path': checkpoint_callback.best_model_path,\n",
        "    }\n",
        "\n",
        "    print(output_json)\n",
        "\n",
        "    return output_json"
      ],
      "metadata": {
        "id": "-6UutjovP0Rl"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# put this and the class above in a separate *.py file for best model training\n",
        "# that means using the `if __name__ == '__main__':` line\n",
        "# with an __init__.py file\n",
        "def training():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data_csv_path\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=500)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train(\n",
        "        data_csv_path=args.data_csv_path,\n",
        "        epochs=args.epochs,\n",
        "    )"
      ],
      "metadata": {
        "id": "INEUAQa4mceH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are tests\n",
        "# Tests are meant to run in a separate *.py file, not in a notebook like this one.\n",
        "\n",
        "# preprocessing test\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture\n",
        "def data():\n",
        "    df = pd.DataFrame({'id': range(1000, 1010)})\n",
        "    return df\n",
        "\n",
        "\n",
        "def test_map_column(data):\n",
        "    df, mapping, inverse_mapping = map_column(data, col_name='id')\n",
        "\n",
        "    assert mapping == {\n",
        "        1000: 2,\n",
        "        1001: 3,\n",
        "        1002: 4,\n",
        "        1003: 5,\n",
        "        1004: 6,\n",
        "        1005: 7,\n",
        "        1006: 8,\n",
        "        1007: 9,\n",
        "        1008: 10,\n",
        "        1009: 11,\n",
        "    }\n",
        "    assert inverse_mapping == {\n",
        "        2: 1000,\n",
        "        3: 1001,\n",
        "        4: 1002,\n",
        "        5: 1003,\n",
        "        6: 1004,\n",
        "        7: 1005,\n",
        "        8: 1006,\n",
        "        9: 1007,\n",
        "        10: 1008,\n",
        "        11: 1009,\n",
        "    }\n",
        "\n",
        "\n",
        "# model test \n",
        "def test_recommender():\n",
        "    n_items = 1000\n",
        "\n",
        "    recommender = Recommender(vocab_size=1000)\n",
        "\n",
        "    src_items = torch.randint(low=0, high=n_items, size=(32, 30))\n",
        "\n",
        "    src_items[:, 0] = 1\n",
        "\n",
        "    trg_out = torch.randint(low=0, high=n_items, size=(32, 30))\n",
        "\n",
        "    out = recommender(src_items)\n",
        "\n",
        "    assert out.shape == torch.Size([32, 30, 1000])\n",
        "\n",
        "    loss = recommender.training_step((src_items, trg_out), batch_idx=1)\n",
        "\n",
        "    assert isinstance(loss, torch.Tensor)\n",
        "\n",
        "    assert not torch.isnan(loss).any()\n",
        "\n",
        "    assert loss.size() == torch.Size([])"
      ],
      "metadata": {
        "id": "LU_sBy3MnDl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD = 0\n",
        "MASK = 1"
      ],
      "metadata": {
        "id": "Eqho5-9bM7Th"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_column(df: pd.DataFrame, col_name: str):\n",
        "    values = sorted(list(df[col_name].unique()))\n",
        "    mapping = {k: i + 2 for i, k in enumerate(values)}\n",
        "    inverse_mapping = {v: k for k, v in mapping.items()}\n",
        "\n",
        "    df[col_name + '_mapped'] = df[col_name].map(mapping)\n",
        "\n",
        "    return df, mapping, inverse_mapping\n",
        "\n",
        "\n",
        "def get_context(df: pd.DataFrame, split: str, context_size: int = 120, val_context_size: int = 5):\n",
        "    if split == 'train':\n",
        "        end_index = random.randint(10, df.shape[0] - val_context_size)\n",
        "    elif split in ['val', 'test']:\n",
        "        end_index = df.shape[0]\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    start_index = max(0, end_index - context_size)\n",
        "\n",
        "    context = df[start_index:end_index]\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "def pad_array(array: np.ndarray, expected_size: int = 30):\n",
        "    array = np.pad(array, [(expected_size - array.shape[0], 0), (0, 0)], mode='edge')\n",
        "    return array\n",
        "\n",
        "\n",
        "def pad_list(list_integers, history_size: int, pad_val: int = PAD, mode='left'):\n",
        "\n",
        "    if len(list_integers) < history_size:\n",
        "        if mode == 'left':\n",
        "            list_integers = [pad_val] * (history_size - len(list_integers)) + list_integers\n",
        "        else:\n",
        "            list_integers = list_integers + [pad_val] * (history_size - len(list_integers))\n",
        "\n",
        "    return list_integers\n",
        "\n",
        "\n",
        "def df_to_np(df, expected_size=30):\n",
        "    array = np.array(df)\n",
        "    array = pad_array(array, expected_size=expected_size)\n",
        "    return array\n",
        "\n",
        "\n",
        "def genome_mapping(genome):\n",
        "    genome.sort_values(by=['movieId', 'tagId'], inplace=True)\n",
        "    movie_genome = genome.groupby('movieId')['relevance'].agg(list).reset_index()\n",
        "\n",
        "    movie_genome = {a: b for a, b in zip(movie_genome['movieId'], movie_genome['relevance'])}\n",
        "\n",
        "    return movie_genome"
      ],
      "metadata": {
        "id": "m3K_qFlVMwfS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/gdrive/MyDrive/movie_recommendation/movie_dataset_bert/ratings.csv'\n",
        "movie_path = '/content/gdrive/MyDrive/movie_recommendation/movie_dataset_bert/movies.csv'"
      ],
      "metadata": {
        "id": "vD1WLDYDMw4V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = '' # https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html\n",
        "# but training is costly so...chugging on with inaccurate results below."
      ],
      "metadata": {
        "id": "Onp1OnAHSfmu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(data_path)\n",
        "movies = pd.read_csv(movie_path)"
      ],
      "metadata": {
        "id": "T-i8A4kPN9Ix"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.sort_values(by='timestamp', inplace=True)"
      ],
      "metadata": {
        "id": "jRxg2XWsOKMm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, mapping, inverse_mapping = map_column(data, col_name = 'movieId')\n",
        "group_by_trains = data.groupby(by='userId')"
      ],
      "metadata": {
        "id": "ZO7Yu7f3ORsH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(group_by_trains.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj5h3KFqctMU",
        "outputId": "313c0803-0d6f-41b0-a226-f9159daecf56"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          userId  movieId  rating   timestamp  movieId_mapped\n",
            "326761      2262       21     3.0   789652009              22\n",
            "326810      2262     1079     3.0   789652009            1054\n",
            "326767      2262       47     5.0   789652009              48\n",
            "15845015  102689        1     4.0   822873600               2\n",
            "15845023  102689       39     5.0   822873600              40\n",
            "...          ...      ...     ...         ...             ...\n",
            "13207835   85523      318     5.0  1574325400             316\n",
            "13207852   85523    58559     4.0  1574325420           12218\n",
            "13207846   85523    33794     4.0  1574325422           10002\n",
            "13207848   85523    44191     4.5  1574325426           10679\n",
            "13207849   85523    48516     4.5  1574325429           11124\n",
            "\n",
            "[812705 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(list(group_by_trains.groups), k = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDGeOA96Oegf",
        "outputId": "4cb1a678-8468-4ff1-d19b-5e90ba320619"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[78869, 28165, 120446, 18011, 25044, 121316, 6741, 31004, 13680, 144183]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Recommender(vocab_size = len(mapping) + 2, dropout = 0.3, lr = 0.0001)\n",
        "model.eval()\n",
        "# model.load_state_dict(torch.load(saved_model)['state_dict']) # load a pre-trained and saved model here\n",
        "# no saved model to load so error is to be expected if uncommented\n",
        "# ...wow...10 samples is bad....just....so bad."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZBLt9MROly_",
        "outputId": "e8c99fe8-7e4b-496c-dcde-a07dcec9a612"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Recommender(\n",
              "  (item_embeddings): Embedding(59049, 128)\n",
              "  (input_pos_embedding): Embedding(512, 128)\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear_out): Linear(in_features=128, out_features=59049, bias=True)\n",
              "  (do): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movie_to_index = {a: mapping[b] for a, b in zip(movies.title.tolist(), movies.movieId.tolist()) if b in mapping}\n",
        "index_to_movie = {v: k for k, v in movie_to_index.items()}"
      ],
      "metadata": {
        "id": "aYr4KJoxSUWS"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(list_movies, model, movie_to_index, index_to_movie):\n",
        "    ids = [PAD] * (120 - len(list_movies) - 1) + [movie_to_index[a] for a in list_movies] + [MASK]\n",
        "    \n",
        "    src = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        prediction = model(src)\n",
        "    \n",
        "    masked_pred = prediction[0, -1].numpy()\n",
        "    \n",
        "    sorted_predicted_ids = np.argsort(masked_pred).tolist()[::-1]\n",
        "    \n",
        "    sorted_predicted_ids = [a for a in sorted_predicted_ids if a not in ids]\n",
        "    \n",
        "    return [index_to_movie[a] for a in sorted_predicted_ids[:30] if a in index_to_movie]"
      ],
      "metadata": {
        "id": "vrkvaqMBTT-w"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_movies = ['Harry Potter and the Sorcerer\\'s Stone (a.k.a. Harry Potter and the Philosopher\\'s Stone) (2001)',\n",
        "               'Harry Potter and the Chamber of Secrets (2002)',\n",
        "               'Harry Potter and the Prisoner of Azkaban (2004)',\n",
        "               'Harry Potter and the Goblet of Fire (2005)']\n",
        "\n",
        "top_movie_adventure_fantasy = predict(list_movies, model, movie_to_index, index_to_movie)\n",
        "top_movie_adventure_fantasy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT0MiZO2TmCv",
        "outputId": "34686838-08d1-45cc-e1f5-5d7a692c6ad3"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Stand Up and Cheer! (1934)',\n",
              " 'Reluctant Saint, The (1962)',\n",
              " 'Fury of Achilles (1962)',\n",
              " 'Le noeud cravate (2008)',\n",
              " 'Take the Trash (2008)',\n",
              " 'Questi fantasmi (1962)',\n",
              " 'Taken (2002)',\n",
              " 'Far Cry (2008)',\n",
              " 'Blue Monkey (1987)',\n",
              " 'Strayed (2009)',\n",
              " 'Who Am I (Kein System Ist Sicher) (2014)',\n",
              " 'Today We Kill, Tomorrow We Die! (1968)',\n",
              " 'My Old Lady (2014)',\n",
              " 'American Outlaws (2001)',\n",
              " 'The Witch Files (2018)',\n",
              " 'Little Galicia (2015)',\n",
              " 'Nomis (2018)',\n",
              " 'Players, The (Les infidèles) (2012)',\n",
              " 'Bardelys the Magnificent (1926)',\n",
              " 'Jumper (2008)',\n",
              " 'Cry in the Dark, A (1988)',\n",
              " \"Rooster's Breakfast (Petelinji zajtrk) (2007)\",\n",
              " 'The Royal Tailor (2014)',\n",
              " 'Daughters Courageous (1939)',\n",
              " 'The Tunnel (1933)',\n",
              " 'The Great Martian War 1913 - 1917 (2013)',\n",
              " 'No Escape (1994)',\n",
              " 'Bis (2015)',\n",
              " 'Tabasco Road (1957)',\n",
              " 'The Parent Trap II (1986)']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_movies = [\"Black Panther (2017)\",\n",
        "               \"Avengers, The (2012)\",\n",
        "               \"Avengers: Infinity War - Part I (2018)\",\n",
        "               \"Logan (2017)\",\n",
        "               \"Spider-Man (2002)\",\n",
        "               \"Spider-Man 3 (2007)\",\n",
        "               \"Spider-Man: Far from Home (2019)\"]\n",
        "top_movie_action_adv = predict(list_movies, model, movie_to_index, index_to_movie)\n",
        "top_movie_action_adv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZFTk945UUGi",
        "outputId": "cb031c3d-c099-4ee2-9331-85f41c91adc9"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Stand Up and Cheer! (1934)',\n",
              " 'Reluctant Saint, The (1962)',\n",
              " 'Fury of Achilles (1962)',\n",
              " 'Le noeud cravate (2008)',\n",
              " 'Take the Trash (2008)',\n",
              " 'Questi fantasmi (1962)',\n",
              " 'Blue Monkey (1987)',\n",
              " 'Taken (2002)',\n",
              " 'Far Cry (2008)',\n",
              " 'Strayed (2009)',\n",
              " 'Who Am I (Kein System Ist Sicher) (2014)',\n",
              " 'Today We Kill, Tomorrow We Die! (1968)',\n",
              " 'My Old Lady (2014)',\n",
              " 'The Tunnel (1933)',\n",
              " 'Little Galicia (2015)',\n",
              " 'Players, The (Les infidèles) (2012)',\n",
              " 'American Outlaws (2001)',\n",
              " 'The Witch Files (2018)',\n",
              " 'Nomis (2018)',\n",
              " 'The Parent Trap II (1986)',\n",
              " \"Rooster's Breakfast (Petelinji zajtrk) (2007)\",\n",
              " 'Bis (2015)',\n",
              " 'Jumper (2008)',\n",
              " 'A Girl Named Sooner (1975)',\n",
              " 'Tabasco Road (1957)',\n",
              " 'Daughters Courageous (1939)',\n",
              " 'No Escape (1994)',\n",
              " 'The Great Martian War 1913 - 1917 (2013)',\n",
              " 'Bardelys the Magnificent (1926)',\n",
              " 'Cry in the Dark, A (1988)']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_movies = [\"Zootopia (2016)\",\n",
        "               \"Toy Story 3 (2010)\",\n",
        "               \"Toy Story 4 (2019)\",\n",
        "               \"Finding Nemo (2003)\",\n",
        "               \"Ratatouille (2007)\",\n",
        "               \"The Lego Movie (2014)\",\n",
        "               \"Ghostbusters (a.k.a. Ghost Busters) (1984)\",\n",
        "               \"Ace Ventura: When Nature Calls (1995)\"]\n",
        "top_movie_comedy = predict(list_movies, model, movie_to_index, index_to_movie)\n",
        "top_movie_comedy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC-EQvhzUzif",
        "outputId": "d9ca3e6a-4edc-470b-f33d-0103d87e6488"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Stand Up and Cheer! (1934)',\n",
              " 'Reluctant Saint, The (1962)',\n",
              " 'Fury of Achilles (1962)',\n",
              " 'Take the Trash (2008)',\n",
              " 'Le noeud cravate (2008)',\n",
              " 'Questi fantasmi (1962)',\n",
              " 'Blue Monkey (1987)',\n",
              " 'Far Cry (2008)',\n",
              " 'Who Am I (Kein System Ist Sicher) (2014)',\n",
              " 'Taken (2002)',\n",
              " 'Strayed (2009)',\n",
              " 'My Old Lady (2014)',\n",
              " 'Today We Kill, Tomorrow We Die! (1968)',\n",
              " 'American Outlaws (2001)',\n",
              " 'Little Galicia (2015)',\n",
              " 'The Tunnel (1933)',\n",
              " 'The Parent Trap II (1986)',\n",
              " 'Players, The (Les infidèles) (2012)',\n",
              " 'Nomis (2018)',\n",
              " 'Bis (2015)',\n",
              " 'Bardelys the Magnificent (1926)',\n",
              " 'A Girl Named Sooner (1975)',\n",
              " 'Cry in the Dark, A (1988)',\n",
              " 'Ardh Satya (1983)',\n",
              " 'The Witch Files (2018)',\n",
              " 'No Escape (1994)',\n",
              " 'Tabasco Road (1957)',\n",
              " 'Jumper (2008)',\n",
              " 'Daughters Courageous (1939)',\n",
              " \"Rooster's Breakfast (Petelinji zajtrk) (2007)\"]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCxXNtHEV0uc",
        "outputId": "e3123f66-9ee2-416e-8f27-3f2e5829fbbd"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How to Use Guys with Secret Tips (2013)',\n",
              " 'More Than a Miracle (1967)',\n",
              " 'Once Upon a Warrior (Anaganaga O Dheerudu) (2011)',\n",
              " 'Inside Man: Most Wanted (2019)',\n",
              " 'Celeste and Jesse Forever (Celeste & Jesse Forever) (2012)',\n",
              " 'Last Unicorn, The (1982)',\n",
              " '48 Christmas Wishes (2017)',\n",
              " 'Piranha II: The Spawning (1981)',\n",
              " 'All Cheerleaders Die (2013)',\n",
              " 'San Francisco International (1970)',\n",
              " 'Big Gold Dream: Scottish Post-Punk and Infiltrating the Mainstream (2015)',\n",
              " 'Brotherhood of Death (1976)',\n",
              " 'P.S. (2004)',\n",
              " 'Pope John Paul II (2005)',\n",
              " 'Malice@Doll (2004)',\n",
              " 'Eddie Izzard: Circle (2002)',\n",
              " \"Everybody's Fine (Stanno tutti bene) (1990)\",\n",
              " \"You Can't Kill Stephen King (2013)\",\n",
              " 'Before the Rain (Pred dozhdot) (1994)',\n",
              " 'That still Karloson! (2012)',\n",
              " 'Asteria (2017)',\n",
              " 'Resina (2018)',\n",
              " 'Gary Gulman: In This Economy? (2012)',\n",
              " 'The Auction (2013)',\n",
              " 'Sleep No More (2018)',\n",
              " 'Tim (1979)',\n",
              " 'The Wild Country (1970)',\n",
              " 'Learning to Lie (2003)',\n",
              " \"McCinsey's Island (1998)\",\n",
              " 'Infiltration (2017)']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kun5mJvchfG"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}