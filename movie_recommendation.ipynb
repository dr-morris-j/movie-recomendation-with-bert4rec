{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElRzQdTOK_Js",
        "outputId": "6eaf8374-b591-4c4b-d6d7-72b02e59ef58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.0.2-py3-none-any.whl (719 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.0/719.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.4.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.27.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (16.0.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch-lightning-2.0.2 torchmetrics-0.11.4 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "%pip install torch\n",
        "%pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True, timeout_ms=120000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnlOy-H3LBUR",
        "outputId": "60f552c7-0f33-4ea7-fa45-2850b23c4856"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/movie_recommendation/movie_dataset_bert/\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtmpukKmLKYV",
        "outputId": "18fcaa79-b4af-41f8-f2de-ed985640a443"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/movie_recommendation/movie_dataset_bert\n",
            "genome-scores.csv  links.csv   ratings.csv  tags.csv\n",
            "genome-tags.csv    movies.csv  README.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn as nn\n",
        "\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import Linear\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "eDdJ2xF1LhoF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_accuracy(y_pred: torch.Tensor, y_true: torch.Tensor, mask: torch.Tensor):\n",
        "\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "    y_true = torch.masked_select(y_true, mask)\n",
        "    predicted = torch.masked_select(predicted, mask)\n",
        "\n",
        "    acc = (y_true == predicted).double().mean()\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def masked_ce(y_pred, y_true, mask):\n",
        "\n",
        "    loss = F.cross_entropy(y_pred, y_true, reduction=\"none\")\n",
        "\n",
        "    loss = loss * mask\n",
        "\n",
        "    return loss.sum() / (mask.sum() + 1e-8)\n",
        "\n",
        "\n",
        "class Recommender(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        channels=128,\n",
        "        cap=0,\n",
        "        mask=1,\n",
        "        dropout=0.4,\n",
        "        lr=1e-4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cap = cap\n",
        "        self.mask = mask\n",
        "\n",
        "        self.lr = lr\n",
        "        self.dropout = dropout\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.item_embeddings = torch.nn.Embedding(\n",
        "            self.vocab_size, embedding_dim=channels\n",
        "        )\n",
        "\n",
        "        self.input_pos_embedding = torch.nn.Embedding(512, embedding_dim=channels)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=channels, nhead=4, dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "\n",
        "        self.linear_out = Linear(channels, self.vocab_size)\n",
        "\n",
        "        self.do = nn.Dropout(p=self.dropout)\n",
        "\n",
        "    def encode_src(self, src_items):\n",
        "        src_items = self.item_embeddings(src_items)\n",
        "\n",
        "        batch_size, in_sequence_len = src_items.size(0), src_items.size(1)\n",
        "        pos_encoder = (\n",
        "            torch.arange(0, in_sequence_len, device=src_items.device)\n",
        "            .unsqueeze(0)\n",
        "            .repeat(batch_size, 1)\n",
        "        )\n",
        "        pos_encoder = self.input_pos_embedding(pos_encoder)\n",
        "\n",
        "        src_items += pos_encoder\n",
        "\n",
        "        src = src_items.permute(1, 0, 2)\n",
        "\n",
        "        src = self.encoder(src)\n",
        "\n",
        "        return src.permute(1, 0, 2)\n",
        "\n",
        "    def forward(self, src_items):\n",
        "\n",
        "        src = self.encode_src(src_items)\n",
        "\n",
        "        out = self.linear_out(src)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, batch_index):\n",
        "        src_items, y_true = batch\n",
        "\n",
        "        y_pred = self(src_items)\n",
        "\n",
        "        y_pred = y_pred.view(-1, y_pred.size(2))\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        src_items = src_items.view(-1)\n",
        "        mask = src_items == self.mask\n",
        "\n",
        "        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_accuracy', accuracy)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_index):\n",
        "        src_items, y_true = batch\n",
        "\n",
        "        y_pred = self(src_items)\n",
        "\n",
        "        y_pred = y_pred.view(-1, y_pred.size(2))\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        src_items = src_items.view(-1)\n",
        "        mask = src_items == self.mask\n",
        "\n",
        "        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "\n",
        "        self.log('valid_loss', loss)\n",
        "        self.log('valid_accuracy', accuracy)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_index):\n",
        "        src_items, y_true = batch\n",
        "\n",
        "        y_pred = self(src_items)\n",
        "\n",
        "        y_pred = y_pred.view(-1, y_pred.size(2))\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        src_items = src_items.view(-1)\n",
        "        mask = src_items == self.mask\n",
        "\n",
        "        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_accuracy', accuracy)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, patience=10, factor=0.1\n",
        "        )\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': scheduler,\n",
        "            'monitor': 'valid_loss',\n",
        "        }"
      ],
      "metadata": {
        "id": "1Ki7hzomMA1O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6UutjovP0Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD = 0\n",
        "MASK = 1"
      ],
      "metadata": {
        "id": "Eqho5-9bM7Th"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_column(df: pd.DataFrame, col_name: str):\n",
        "    values = sorted(list(df[col_name].unique()))\n",
        "    mapping = {k: i + 2 for i, k in enumerate(values)}\n",
        "    inverse_mapping = {v: k for k, v in mapping.items()}\n",
        "\n",
        "    df[col_name + '_mapped'] = df[col_name].map(mapping)\n",
        "\n",
        "    return df, mapping, inverse_mapping\n",
        "\n",
        "\n",
        "def get_context(df: pd.DataFrame, split: str, context_size: int = 120, val_context_size: int = 5):\n",
        "    if split == 'train':\n",
        "        end_index = random.randint(10, df.shape[0] - val_context_size)\n",
        "    elif split in ['val', 'test']:\n",
        "        end_index = df.shape[0]\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    start_index = max(0, end_index - context_size)\n",
        "\n",
        "    context = df[start_index:end_index]\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "def pad_array(array: np.ndarray, expected_size: int = 30):\n",
        "    array = np.pad(array, [(expected_size - array.shape[0], 0), (0, 0)], mode='edge')\n",
        "    return array\n",
        "\n",
        "\n",
        "def pad_list(list_integers, history_size: int, pad_val: int = PAD, mode='left'):\n",
        "\n",
        "    if len(list_integers) < history_size:\n",
        "        if mode == 'left':\n",
        "            list_integers = [pad_val] * (history_size - len(list_integers)) + list_integers\n",
        "        else:\n",
        "            list_integers = list_integers + [pad_val] * (history_size - len(list_integers))\n",
        "\n",
        "    return list_integers\n",
        "\n",
        "\n",
        "def df_to_np(df, expected_size=30):\n",
        "    array = np.array(df)\n",
        "    array = pad_array(array, expected_size=expected_size)\n",
        "    return array\n",
        "\n",
        "\n",
        "def genome_mapping(genome):\n",
        "    genome.sort_values(by=['movieId', 'tagId'], inplace=True)\n",
        "    movie_genome = genome.groupby('movieId')['relevance'].agg(list).reset_index()\n",
        "\n",
        "    movie_genome = {a: b for a, b in zip(movie_genome['movieId'], movie_genome['relevance'])}\n",
        "\n",
        "    return movie_genome"
      ],
      "metadata": {
        "id": "m3K_qFlVMwfS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/gdrive/MyDrive/movie_recommendation/movie_dataset_bert/ratings.csv'\n",
        "movie_path = '/content/gdrive/MyDrive/movie_recommendation/movie_dataset_bert/movies.csv'"
      ],
      "metadata": {
        "id": "vD1WLDYDMw4V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = '' # https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html\n",
        "# but training is costly so...chugging on with inaccurate results below."
      ],
      "metadata": {
        "id": "Onp1OnAHSfmu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(data_path)\n",
        "movies = pd.read_csv(movie_path)"
      ],
      "metadata": {
        "id": "T-i8A4kPN9Ix"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.sort_values(by='timestamp', inplace=True)"
      ],
      "metadata": {
        "id": "jRxg2XWsOKMm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, mapping, inverse_mapping = map_column(data, col_name = 'movieId')\n",
        "group_by_trains = data.groupby(by='userId')"
      ],
      "metadata": {
        "id": "ZO7Yu7f3ORsH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(group_by_trains.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj5h3KFqctMU",
        "outputId": "313c0803-0d6f-41b0-a226-f9159daecf56"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          userId  movieId  rating   timestamp  movieId_mapped\n",
            "326761      2262       21     3.0   789652009              22\n",
            "326810      2262     1079     3.0   789652009            1054\n",
            "326767      2262       47     5.0   789652009              48\n",
            "15845015  102689        1     4.0   822873600               2\n",
            "15845023  102689       39     5.0   822873600              40\n",
            "...          ...      ...     ...         ...             ...\n",
            "13207835   85523      318     5.0  1574325400             316\n",
            "13207852   85523    58559     4.0  1574325420           12218\n",
            "13207846   85523    33794     4.0  1574325422           10002\n",
            "13207848   85523    44191     4.5  1574325426           10679\n",
            "13207849   85523    48516     4.5  1574325429           11124\n",
            "\n",
            "[812705 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(list(group_by_trains.groups), k = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDGeOA96Oegf",
        "outputId": "4cb1a678-8468-4ff1-d19b-5e90ba320619"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[78869, 28165, 120446, 18011, 25044, 121316, 6741, 31004, 13680, 144183]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Recommender(vocab_size = len(mapping) + 2, dropout = 0.3, lr = 0.0001)\n",
        "model.eval()\n",
        "# model.load_state_dict(torch.load(saved_model)['state_dict']) # load a pre-trained and saved model here\n",
        "# no saved model to load so error is to be expected if uncommented\n",
        "# ...wow...10 samples is bad....just....so bad."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZBLt9MROly_",
        "outputId": "e8c99fe8-7e4b-496c-dcde-a07dcec9a612"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Recommender(\n",
              "  (item_embeddings): Embedding(59049, 128)\n",
              "  (input_pos_embedding): Embedding(512, 128)\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear_out): Linear(in_features=128, out_features=59049, bias=True)\n",
              "  (do): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movie_to_index = {a: mapping[b] for a, b in zip(movies.title.tolist(), movies.movieId.tolist()) if b in mapping}\n",
        "index_to_movie = {v: k for k, v in movie_to_index.items()}"
      ],
      "metadata": {
        "id": "aYr4KJoxSUWS"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(list_movies, model, movie_to_index, index_to_movie):\n",
        "    ids = [PAD] * (120 - len(list_movies) - 1) + [movie_to_index[a] for a in list_movies] + [MASK]\n",
        "    \n",
        "    src = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        prediction = model(src)\n",
        "    \n",
        "    masked_pred = prediction[0, -1].numpy()\n",
        "    \n",
        "    sorted_predicted_ids = np.argsort(masked_pred).tolist()[::-1]\n",
        "    \n",
        "    sorted_predicted_ids = [a for a in sorted_predicted_ids if a not in ids]\n",
        "    \n",
        "    return [index_to_movie[a] for a in sorted_predicted_ids[:30] if a in index_to_movie]"
      ],
      "metadata": {
        "id": "vrkvaqMBTT-w"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_movies = ['Harry Potter and the Sorcerer\\'s Stone (a.k.a. Harry Potter and the Philosopher\\'s Stone) (2001)',\n",
        "               'Harry Potter and the Chamber of Secrets (2002)',\n",
        "               'Harry Potter and the Prisoner of Azkaban (2004)',\n",
        "               'Harry Potter and the Goblet of Fire (2005)']\n",
        "\n",
        "top_movie_adventure_fantasy = predict(list_movies, model, movie_to_index, index_to_movie)\n",
        "top_movie_adventure_fantasy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT0MiZO2TmCv",
        "outputId": "34686838-08d1-45cc-e1f5-5d7a692c6ad3"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Stand Up and Cheer! (1934)',\n",
              " 'Reluctant Saint, The (1962)',\n",
              " 'Fury of Achilles (1962)',\n",
              " 'Le noeud cravate (2008)',\n",
              " 'Take the Trash (2008)',\n",
              " 'Questi fantasmi (1962)',\n",
              " 'Taken (2002)',\n",
              " 'Far Cry (2008)',\n",
              " 'Blue Monkey (1987)',\n",
              " 'Strayed (2009)',\n",
              " 'Who Am I (Kein System Ist Sicher) (2014)',\n",
              " 'Today We Kill, Tomorrow We Die! (1968)',\n",
              " 'My Old Lady (2014)',\n",
              " 'American Outlaws (2001)',\n",
              " 'The Witch Files (2018)',\n",
              " 'Little Galicia (2015)',\n",
              " 'Nomis (2018)',\n",
              " 'Players, The (Les infidèles) (2012)',\n",
              " 'Bardelys the Magnificent (1926)',\n",
              " 'Jumper (2008)',\n",
              " 'Cry in the Dark, A (1988)',\n",
              " \"Rooster's Breakfast (Petelinji zajtrk) (2007)\",\n",
              " 'The Royal Tailor (2014)',\n",
              " 'Daughters Courageous (1939)',\n",
              " 'The Tunnel (1933)',\n",
              " 'The Great Martian War 1913 - 1917 (2013)',\n",
              " 'No Escape (1994)',\n",
              " 'Bis (2015)',\n",
              " 'Tabasco Road (1957)',\n",
              " 'The Parent Trap II (1986)']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_movies = [\"Black Panther (2017)\",\n",
        "               \"Avengers, The (2012)\",\n",
        "               \"Avengers: Infinity War - Part I (2018)\",\n",
        "               \"Logan (2017)\",\n",
        "               \"Spider-Man (2002)\",\n",
        "               \"Spider-Man 3 (2007)\",\n",
        "               \"Spider-Man: Far from Home (2019)\"]\n",
        "top_movie_action_adv = predict(list_movies, model, movie_to_index, index_to_movie)\n",
        "top_movie_action_adv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZFTk945UUGi",
        "outputId": "cb031c3d-c099-4ee2-9331-85f41c91adc9"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Stand Up and Cheer! (1934)',\n",
              " 'Reluctant Saint, The (1962)',\n",
              " 'Fury of Achilles (1962)',\n",
              " 'Le noeud cravate (2008)',\n",
              " 'Take the Trash (2008)',\n",
              " 'Questi fantasmi (1962)',\n",
              " 'Blue Monkey (1987)',\n",
              " 'Taken (2002)',\n",
              " 'Far Cry (2008)',\n",
              " 'Strayed (2009)',\n",
              " 'Who Am I (Kein System Ist Sicher) (2014)',\n",
              " 'Today We Kill, Tomorrow We Die! (1968)',\n",
              " 'My Old Lady (2014)',\n",
              " 'The Tunnel (1933)',\n",
              " 'Little Galicia (2015)',\n",
              " 'Players, The (Les infidèles) (2012)',\n",
              " 'American Outlaws (2001)',\n",
              " 'The Witch Files (2018)',\n",
              " 'Nomis (2018)',\n",
              " 'The Parent Trap II (1986)',\n",
              " \"Rooster's Breakfast (Petelinji zajtrk) (2007)\",\n",
              " 'Bis (2015)',\n",
              " 'Jumper (2008)',\n",
              " 'A Girl Named Sooner (1975)',\n",
              " 'Tabasco Road (1957)',\n",
              " 'Daughters Courageous (1939)',\n",
              " 'No Escape (1994)',\n",
              " 'The Great Martian War 1913 - 1917 (2013)',\n",
              " 'Bardelys the Magnificent (1926)',\n",
              " 'Cry in the Dark, A (1988)']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_movies = [\"Zootopia (2016)\",\n",
        "               \"Toy Story 3 (2010)\",\n",
        "               \"Toy Story 4 (2019)\",\n",
        "               \"Finding Nemo (2003)\",\n",
        "               \"Ratatouille (2007)\",\n",
        "               \"The Lego Movie (2014)\",\n",
        "               \"Ghostbusters (a.k.a. Ghost Busters) (1984)\",\n",
        "               \"Ace Ventura: When Nature Calls (1995)\"]\n",
        "top_movie_comedy = predict(list_movies, model, movie_to_index, index_to_movie)\n",
        "top_movie_comedy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC-EQvhzUzif",
        "outputId": "d9ca3e6a-4edc-470b-f33d-0103d87e6488"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Stand Up and Cheer! (1934)',\n",
              " 'Reluctant Saint, The (1962)',\n",
              " 'Fury of Achilles (1962)',\n",
              " 'Take the Trash (2008)',\n",
              " 'Le noeud cravate (2008)',\n",
              " 'Questi fantasmi (1962)',\n",
              " 'Blue Monkey (1987)',\n",
              " 'Far Cry (2008)',\n",
              " 'Who Am I (Kein System Ist Sicher) (2014)',\n",
              " 'Taken (2002)',\n",
              " 'Strayed (2009)',\n",
              " 'My Old Lady (2014)',\n",
              " 'Today We Kill, Tomorrow We Die! (1968)',\n",
              " 'American Outlaws (2001)',\n",
              " 'Little Galicia (2015)',\n",
              " 'The Tunnel (1933)',\n",
              " 'The Parent Trap II (1986)',\n",
              " 'Players, The (Les infidèles) (2012)',\n",
              " 'Nomis (2018)',\n",
              " 'Bis (2015)',\n",
              " 'Bardelys the Magnificent (1926)',\n",
              " 'A Girl Named Sooner (1975)',\n",
              " 'Cry in the Dark, A (1988)',\n",
              " 'Ardh Satya (1983)',\n",
              " 'The Witch Files (2018)',\n",
              " 'No Escape (1994)',\n",
              " 'Tabasco Road (1957)',\n",
              " 'Jumper (2008)',\n",
              " 'Daughters Courageous (1939)',\n",
              " \"Rooster's Breakfast (Petelinji zajtrk) (2007)\"]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCxXNtHEV0uc",
        "outputId": "e3123f66-9ee2-416e-8f27-3f2e5829fbbd"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How to Use Guys with Secret Tips (2013)',\n",
              " 'More Than a Miracle (1967)',\n",
              " 'Once Upon a Warrior (Anaganaga O Dheerudu) (2011)',\n",
              " 'Inside Man: Most Wanted (2019)',\n",
              " 'Celeste and Jesse Forever (Celeste & Jesse Forever) (2012)',\n",
              " 'Last Unicorn, The (1982)',\n",
              " '48 Christmas Wishes (2017)',\n",
              " 'Piranha II: The Spawning (1981)',\n",
              " 'All Cheerleaders Die (2013)',\n",
              " 'San Francisco International (1970)',\n",
              " 'Big Gold Dream: Scottish Post-Punk and Infiltrating the Mainstream (2015)',\n",
              " 'Brotherhood of Death (1976)',\n",
              " 'P.S. (2004)',\n",
              " 'Pope John Paul II (2005)',\n",
              " 'Malice@Doll (2004)',\n",
              " 'Eddie Izzard: Circle (2002)',\n",
              " \"Everybody's Fine (Stanno tutti bene) (1990)\",\n",
              " \"You Can't Kill Stephen King (2013)\",\n",
              " 'Before the Rain (Pred dozhdot) (1994)',\n",
              " 'That still Karloson! (2012)',\n",
              " 'Asteria (2017)',\n",
              " 'Resina (2018)',\n",
              " 'Gary Gulman: In This Economy? (2012)',\n",
              " 'The Auction (2013)',\n",
              " 'Sleep No More (2018)',\n",
              " 'Tim (1979)',\n",
              " 'The Wild Country (1970)',\n",
              " 'Learning to Lie (2003)',\n",
              " \"McCinsey's Island (1998)\",\n",
              " 'Infiltration (2017)']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kun5mJvchfG"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}